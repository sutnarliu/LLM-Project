# -*- coding: utf-8 -*-
"""
train_qwen.py - ç”¨Qwen-1.8Bè®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼ˆä»ç¼“å­˜åŠ è½½ï¼‰
"""

import os
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

# ============ è®¾ç½®è®¾å¤‡ ============
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("="*60)
print("ğŸš€ å¼€å§‹è®­ç»ƒ Qwen-1.8B ä¸­æ–‡æ¨¡å‹")
print("="*60)
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print("="*60)

# ============ åŠ è½½æ•°æ® ============
data_path = r"C:\Users\LJA\Desktop\LLM-Project\data\train_data.json"
print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")

if not os.path.exists(data_path):
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    exit(1)

with open(data_path, 'r', encoding='utf-8') as f:
    train_examples = json.load(f)

print(f"âœ… åŠ è½½äº† {len(train_examples)} æ¡è®­ç»ƒæ•°æ®")
print("\næ•°æ®ç¤ºä¾‹:")
for i in range(min(3, len(train_examples))):
    print(f"  {i+1}. é—®é¢˜: {train_examples[i]['instruction']}")
    print(f"     ç­”æ¡ˆ: {train_examples[i]['output'][:50]}...")

# ============ æ ¼å¼åŒ–æ•°æ® ============
print("\nğŸ“ æ ¼å¼åŒ–æ•°æ®...")
formatted_texts = []
for ex in train_examples:
    # Qwençš„å¯¹è¯æ ¼å¼
    text = f"<|im_start|>user\n{ex['instruction']}<|im_end|>\n<|im_start|>assistant\n{ex['output']}<|im_end|>"
    formatted_texts.append(text)

# ============ ğŸ”¥ ä¿®æ”¹çš„åœ°æ–¹ï¼šåŠ è½½Qwenæ¨¡å‹ï¼ˆä»ç¼“å­˜ï¼‰============
print("\nğŸ”„ ä»ç¼“å­˜åŠ è½½å·²ä¸‹è½½çš„ Qwen-1.8B æ¨¡å‹...")

model_name = "Qwen/Qwen1.5-1.8B"

try:
    # ç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼ˆlocal_files_only=True å¼ºåˆ¶åªä»æœ¬åœ°åŠ è½½ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, 
        trust_remote_code=True,
        local_files_only=True  # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype="float16",  # ç”¨å­—ç¬¦ä¸²å½¢å¼é¿å…è­¦å‘Š
        device_map="auto",
        local_files_only=True  # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼
    )
    print("âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ ä»ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
    print("è¯·å…ˆç¡®è®¤æ¨¡å‹å·²ä¸‹è½½å®Œæˆ")
    exit(1)

# è®¾ç½®padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")

# ============ Tokenize ============
print("\nğŸ”„ æ­£åœ¨tokenizeæ•°æ®...")

def tokenize_function(examples):
    outputs = tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    )
    
    outputs["labels"] = outputs["input_ids"].clone()
    outputs["labels"][outputs["labels"] == tokenizer.pad_token_id] = -100
    return outputs

# åˆ›å»ºdataset
dataset = Dataset.from_dict({"text": formatted_texts})
tokenized_dataset = dataset.map(
    tokenize_function, 
    batched=True,
    remove_columns=["text"]
)

print(f"âœ… Tokenizeå®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")

# ============ é…ç½®LoRA ============
print("\nâš™ï¸ é…ç½®LoRA...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ============ é…ç½®è®­ç»ƒå‚æ•° ============
print("\nğŸ‹ï¸ é…ç½®è®­ç»ƒå‚æ•°...")

# ä¿å­˜è·¯å¾„
save_path = r"C:\Users\LJA\Desktop\LLM-Project\models\qwen_lora_v1"
os.makedirs(save_path, exist_ok=True)

training_args = TrainingArguments(
    output_dir=save_path,
    num_train_epochs=20,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    report_to="none",
    remove_unused_columns=False,
    dataloader_num_workers=0,
)

# åˆ›å»ºdata collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# ============ å¼€å§‹è®­ç»ƒ ============
print("\nğŸš€ å¼€å§‹è®­ç»ƒ Qwen-1.8B...")
print("è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦20-30åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
print("-" * 40)

trainer.train()

print("\nâœ… è®­ç»ƒå®Œæˆï¼")
print("-" * 40)

# ============ ä¿å­˜æ¨¡å‹ ============
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

# ============ éªŒè¯ä¿å­˜ ============
print("\nğŸ” éªŒè¯ä¿å­˜ç»“æœ...")
files = os.listdir(save_path)
print(f"ğŸ“ æ–‡ä»¶å¤¹å†…å®¹ ({len(files)} ä¸ªæ–‡ä»¶):")
for file in sorted(files):
    file_path = os.path.join(save_path, file)
    if os.path.isfile(file_path):
        size = os.path.getsize(file_path) / 1024
        print(f"   - {file} ({size:.1f} KB)")

print("\n" + "="*60)
print("ğŸ‰ Qwen-1.8B æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“‚ æ¨¡å‹ä½ç½®: {save_path}")
print("="*60)