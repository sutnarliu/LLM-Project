import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ============ 1. è®¾ç½®é•œåƒæºï¼ˆé˜²æ­¢ä»»ä½•è”ç½‘è¯·æ±‚ï¼‰============
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # å¤‡ç”¨
os.environ['TRANSFORMERS_OFFLINE'] = '1'  # ğŸ”¥ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼
os.environ['HF_HUB_OFFLINE'] = '1'        # ğŸ”¥ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼

# ============ 2. åŠ è½½æ¨¡å‹ ============
model_path = r"C:\Users\LJA\Desktop\LLM-Project\models\lora_distilgpt2"
base_model_name = "distilgpt2"

print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
print("=" * 60)

# ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆæ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰ç¼“å­˜
try:
    # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, local_files_only=True)
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, local_files_only=True)
    print("âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½æˆåŠŸï¼")
except:
    print("âš ï¸ æœ¬åœ°æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½ä¸€æ¬¡...")
    # è®¾ç½®é•œåƒæºä¸‹è½½
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
    print("âœ… ä¸‹è½½å®Œæˆï¼Œä¸‹æ¬¡å°±å¯ä»¥ç¦»çº¿ä½¿ç”¨äº†ï¼")

# è®¾ç½®pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(base_model, model_path)

# ç§»åŠ¨åˆ°GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è¿è¡Œè®¾å¤‡: {device}")
print("=" * 60)

# ============ 3. å®šä¹‰é—®ç­”å‡½æ•° ============
def ask_model(question):
    prompt = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "ç­”æ¡ˆï¼š" in response:
        answer = response.split("ç­”æ¡ˆï¼š")[-1]
    else:
        answer = response.replace(prompt, "")
    
    return answer.strip()

# ============ 4. æµ‹è¯•æ•ˆæœ ============
test_questions = [
    "ä»€ä¹ˆæ˜¯æ—¶é—´å¤æ‚åº¦ï¼Ÿ",
    "æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯æ­»é”ï¼Ÿ",
]

print("\nğŸ¤– æ¨¡å‹æµ‹è¯•ç»“æœ:")
print("=" * 60)

for q in test_questions:
    answer = ask_model(q)
    print(f"ğŸ“Œ é—®é¢˜: {q}")
    print(f"ğŸ’¬ å›ç­”: {answer}")
    print("-" * 40)