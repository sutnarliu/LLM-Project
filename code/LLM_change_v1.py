import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # ğŸ”¥ å…³é”®ï¼è§£å†³ç½‘ç»œé—®é¢˜

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

# ============ 1. åœ¨æ¡Œé¢åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹ ============
desktop = os.path.join(os.path.expanduser('~'), 'Desktop')
project_path = os.path.join(desktop, 'LLM-Project')
model_save_path = os.path.join(project_path, 'models', 'lora_distilgpt2')
os.makedirs(model_save_path, exist_ok=True)

print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_save_path}")
print("=" * 60)

# ============ 2. åŠ è½½æ¨¡å‹ï¼ˆç°åœ¨ä¼šä»é•œåƒä¸‹è½½ï¼‰============
print("ğŸ”„ åŠ è½½æ¨¡å‹ï¼ˆä»å›½å†…é•œåƒï¼‰...")
model_name = "distilgpt2"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    print("ğŸ’¡ å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜æˆ–æ›´æ¢æ¨¡å‹...")
    raise

# è®¾ç½®pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ç§»åŠ¨åˆ°GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
print(f"âœ… è¿è¡Œè®¾å¤‡: {device}")
print("=" * 60)

# ============ 3. å‡†å¤‡è®­ç»ƒæ•°æ® ============
print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")

train_examples = [
    {"instruction": "ä»€ä¹ˆæ˜¯æ—¶é—´å¤æ‚åº¦ï¼Ÿ", "output": "æ—¶é—´å¤æ‚åº¦æ˜¯ç®—æ³•æ‰§è¡Œæ—¶é—´éšè¾“å…¥è§„æ¨¡å¢é•¿çš„é‡åº¦ã€‚"},
    {"instruction": "è§£é‡Šä¸€ä¸‹æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«", "output": "æ ˆæ˜¯åè¿›å…ˆå‡ºï¼ˆLIFOï¼‰ï¼Œé˜Ÿåˆ—æ˜¯å…ˆè¿›å…ˆå‡ºï¼ˆFIFOï¼‰ã€‚"},
    {"instruction": "ä»€ä¹ˆæ˜¯æ­»é”ï¼Ÿ", "output": "æ­»é”æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªè¿›ç¨‹äº’ç›¸ç­‰å¾…èµ„æºï¼Œå¯¼è‡´éƒ½æ— æ³•ç»§ç»­æ‰§è¡Œçš„çŠ¶æ€ã€‚"},
    {"instruction": "TCPå’ŒUDPæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ", "output": "TCPé¢å‘è¿æ¥ã€å¯é ï¼›UDPæ— è¿æ¥ã€é€Ÿåº¦å¿«ã€‚"},
    {"instruction": "ä»€ä¹ˆæ˜¯è™šæ‹Ÿå†…å­˜ï¼Ÿ", "output": "è™šæ‹Ÿå†…å­˜æ˜¯æŠŠç£ç›˜ç©ºé—´å½“å†…å­˜ç”¨ï¼Œè®©ç¨‹åºæ‹¥æœ‰å¤§äºç‰©ç†å†…å­˜çš„åœ°å€ç©ºé—´ã€‚"},
]

formatted_texts = []
for ex in train_examples:
    text = f"é—®é¢˜ï¼š{ex['instruction']}\nç­”æ¡ˆï¼š{ex['output']}"
    formatted_texts.append(text)

def tokenize_function(examples):
    inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    inputs["labels"] = inputs["input_ids"].clone()
    return inputs

dataset = Dataset.from_dict({"text": formatted_texts})
tokenized_dataset = dataset.map(tokenize_function, batched=True)

print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå…± {len(tokenized_dataset)} æ¡æ ·æœ¬")
print("=" * 60)

# ============ 4. é…ç½®LoRA ============
print("âš™ï¸ é…ç½®LoRA...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=4,
    lora_alpha=16,
    target_modules=["c_attn"],
    lora_dropout=0.1,
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print("=" * 60)

# ============ 5. è®­ç»ƒé…ç½® ============
print("ğŸ‹ï¸ é…ç½®è®­ç»ƒå‚æ•°...")

training_args = TrainingArguments(
    output_dir=model_save_path,
    num_train_epochs=20,
    per_device_train_batch_size=4,
    logging_steps=5,
    save_strategy="epoch",
    save_total_limit=2,
    learning_rate=5e-4,
    fp16=True,
    report_to="none",
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    #tokenizer=tokenizer,
)

# ============ 6. å¼€å§‹è®­ç»ƒ ============
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("=" * 60)

# ============ 7. å¼ºåˆ¶ä¿å­˜æ¨¡å‹ ============
print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
print("=" * 60)

# ============ 8. éªŒè¯ä¿å­˜ç»“æœ ============
print("ğŸ” éªŒè¯ä¿å­˜ç»“æœ...")
files = os.listdir(model_save_path)
print(f"ğŸ“ æ–‡ä»¶å¤¹å†…å®¹ ({len(files)} ä¸ªæ–‡ä»¶):")

required_files = ['adapter_model.bin', 'adapter_config.json']
for file in sorted(files):
    file_path = os.path.join(model_save_path, file)
    size = os.path.getsize(file_path) / 1024
    print(f"   - {file} ({size:.1f} KB)")

if all(f in files for f in required_files):
    print("\nâœ¨ æ¨¡å‹ä¿å­˜æˆåŠŸï¼å®Œæ•´å¯ç”¨ï¼")
    print(f"ğŸ“‚ è·¯å¾„: {model_save_path}")
else:
    print("\nâŒ ä¿å­˜ä¸å®Œæ•´")

print("\n" + "=" * 60)
print("ğŸ‰ å®Œæˆï¼ä»¥ååŠ è½½æ¨¡å‹å°±ç”¨è¿™ä¸ªè·¯å¾„:")
print(f'model_path = r"{model_save_path}"')