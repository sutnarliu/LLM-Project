import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ============ ç¦»çº¿æ¨¡å¼ ============
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'

# ============ è·¯å¾„è®¾ç½® ============
base_path = r"C:\Users\LJA\Desktop\LLM-Project\models\base_models\distilgpt2"
lora_path = r"C:\Users\LJA\Desktop\LLM-Project\models\v2_60data"

print("="*60)
print("ğŸ§ª æµ‹è¯• v2 æ¨¡å‹ (60æ¡æ•°æ®)")
print("="*60)

# ============ åŠ è½½æ¨¡å‹ ============
print("ğŸ”„ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(base_path, local_files_only=True)
base_model = AutoModelForCausalLM.from_pretrained(base_path, local_files_only=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = PeftModel.from_pretrained(base_model, lora_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()
print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}")

# ============ é—®ç­”å‡½æ•° ============
def ask(question):
    prompt = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "ç­”æ¡ˆï¼š" in response:
        return response.split("ç­”æ¡ˆï¼š")[-1].strip()
    return response.strip()

# ============ æµ‹è¯•é—®é¢˜ ============
test_questions = [
    "ä»€ä¹ˆæ˜¯æ—¶é—´å¤æ‚åº¦ï¼Ÿ",
    "æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯æ­»é”ï¼Ÿ",
    "TCPå’ŒUDPçš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯è™šæ‹Ÿå†…å­˜ï¼Ÿ",
    "è¿›ç¨‹å’Œçº¿ç¨‹çš„åŒºåˆ«"
]

print("\nğŸ“ æµ‹è¯•ç»“æœ:")
print("="*60)

for q in test_questions:
    answer = ask(q)
    print(f"ğŸ“Œ é—®é¢˜: {q}")
    print(f"ğŸ’¬ å›ç­”: {answer}")
    print("-"*40)

# ============ å¯¹æ¯”v1æ¨¡å‹ï¼ˆå¯é€‰ï¼‰============
try:
    v1_path = r"C:\Users\LJA\Desktop\LLM-Project\models\lora_models\v1_5data"
    if os.path.exists(v1_path):
        print("\nğŸ”„ åŠ è½½v1æ¨¡å‹è¿›è¡Œå¯¹æ¯”...")
        model_v1 = PeftModel.from_pretrained(base_model, v1_path).to(device)
        model_v1.eval()
        
        print("\nğŸ“Š v1 (5æ¡) vs v2 (60æ¡) å¯¹æ¯”:")
        print("="*60)
        for q in test_questions[:2]:  # åªå¯¹æ¯”å‰ä¸¤ä¸ª
            answer_v1 = ask_with_model(model_v1, q)
            answer_v2 = ask_with_model(model, q)
            print(f"é—®é¢˜: {q}")
            print(f"v1: {answer_v1[:50]}...")
            print(f"v2: {answer_v2[:50]}...")
            print("-"*40)
except:
    pass

print("\nâœ… æµ‹è¯•å®Œæˆï¼")