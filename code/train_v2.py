import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ============ ç¦»çº¿æ¨¡å¼ ============
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'

# ============ ä½ çš„å®é™…è·¯å¾„ï¼ˆå…ˆç”¨os.pathå¤„ç†ï¼‰============
base_path = os.path.join("C:", os.sep, "Users", "LJA", "Desktop", "LLM-Project", "models", "base_models", "distilgpt2")
lora_path = r"C:\Users\LJA\Desktop\LLM-Project\models\v2_60data"

print("="*60)
print("ğŸ§ª æµ‹è¯• v2 æ¨¡å‹ (60æ¡æ•°æ®)")
print(f"ğŸ“‚ åŸºç¡€æ¨¡å‹è·¯å¾„: {base_path}")
print(f"ğŸ“‚ LoRAæ¨¡å‹è·¯å¾„: {lora_path}")
print("="*60)

# ============ æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ ============
if not os.path.exists(base_path):
    print(f"âš ï¸ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_path}")
    print("å°è¯•ä»ç¼“å­˜åŠ è½½ distilgpt2...")
    try:
        tokenizer = AutoTokenizer.from_pretrained("distilgpt2", local_files_only=True)
        base_model = AutoModelForCausalLM.from_pretrained("distilgpt2", local_files_only=True)
        print("âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸï¼")
    except:
        print("âŒ ç¼“å­˜ä¸­ä¹Ÿæ‰¾ä¸åˆ°ï¼Œéœ€è¦è”ç½‘ä¸‹è½½ä¸€æ¬¡")
        # ä¸´æ—¶å…è®¸è”ç½‘ä¸‹è½½
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        # ä¿å­˜åˆ°ä½ çš„base_modelsç›®å½•
        os.makedirs(base_path, exist_ok=True)
        tokenizer.save_pretrained(base_path)
        base_model.save_pretrained(base_path)
        print(f"âœ… å·²ä¸‹è½½å¹¶ä¿å­˜åˆ°: {base_path}")
else:
    print("ğŸ”„ ä»æœ¬åœ°è·¯å¾„åŠ è½½åŸºç¡€æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(base_path, local_files_only=True)
    base_model = AutoModelForCausalLM.from_pretrained(base_path, local_files_only=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ============ åŠ è½½LoRAæƒé‡ ============
print("ğŸ”„ åŠ è½½LoRAæƒé‡...")
if not os.path.exists(lora_path):
    print(f"âŒ LoRAæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
    exit(1)

model = PeftModel.from_pretrained(base_model, lora_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()
print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}")

# ============ é—®ç­”å‡½æ•° ============
def ask(question):
    prompt = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,           # å¢åŠ ç”Ÿæˆé•¿åº¦
            temperature=0.3,               # é™ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
            do_sample=False,                # å…³é—­é‡‡æ ·ï¼Œæ¯æ¬¡éƒ½é€‰æœ€å¯èƒ½çš„è¯
            repetition_penalty=1.2,         # å¢åŠ é‡å¤æƒ©ç½š
            num_beams=3,                    # ä½¿ç”¨beam search
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å®Œæ•´è¾“å‡º: {response}")  # è°ƒè¯•ç”¨
    if "ç­”æ¡ˆï¼š" in response:
        return response.split("ç­”æ¡ˆï¼š")[-1].strip()
    return response.strip()

# ============ æµ‹è¯•é—®é¢˜ ============
test_questions = [
    "ä»€ä¹ˆæ˜¯æ—¶é—´å¤æ‚åº¦ï¼Ÿ",
    "æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯æ­»é”ï¼Ÿ",
    "TCPå’ŒUDPçš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯è™šæ‹Ÿå†…å­˜ï¼Ÿ",
    "è¿›ç¨‹å’Œçº¿ç¨‹çš„åŒºåˆ«"
]

print("\nğŸ“ æµ‹è¯•ç»“æœ:")
print("="*60)

for q in test_questions:
    answer = ask(q)
    print(f"ğŸ“Œ é—®é¢˜: {q}")
    print(f"ğŸ’¬ å›ç­”: {answer}")
    print("-"*40)

print("\nâœ… æµ‹è¯•å®Œæˆï¼")