# -*- coding: utf-8 -*-
"""
test_qwen_lora.py - æµ‹è¯•è®­ç»ƒå¥½çš„ Qwen-LoRA æ¨¡å‹
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ============ è®¾ç½®è·¯å¾„ ============
# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä½ ç§»åŠ¨åçš„ä½ç½®ï¼Œå¦‚æœæ²¡æœ‰ç§»åŠ¨å°±ç”¨ç¼“å­˜ï¼‰
base_model_path = r"C:\Users\LJA\Desktop\LLM-Project\models\base_models\Qwen_Qwen1.5-1.8B"
lora_path = r"C:\Users\LJA\Desktop\LLM-Project\models\qwen_lora_v1"

# å¦‚æœåŸºç¡€æ¨¡å‹æ²¡ç§»åŠ¨ï¼Œå°±ä»ç¼“å­˜åŠ è½½
if not os.path.exists(base_model_path):
    base_model_path = "Qwen/Qwen1.5-1.8B"
    print("âš ï¸ ä½¿ç”¨ç¼“å­˜ä¸­çš„åŸºç¡€æ¨¡å‹")

print("="*60)
print("ğŸ§ª æµ‹è¯• Qwen-1.8B LoRA å¾®è°ƒæ¨¡å‹")
print(f"ğŸ“‚ LoRA è·¯å¾„: {lora_path}")
print("="*60)

# ============ åŠ è½½åŸºç¡€æ¨¡å‹ ============
print("\nğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path, 
        trust_remote_code=True,
        local_files_only=True
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",
        local_files_only=True
    )
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    exit(1)

# è®¾ç½® padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ============ åŠ è½½ LoRA æƒé‡ ============
print("\nğŸ”„ åŠ è½½ LoRA æƒé‡...")
try:
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.eval()
    print("âœ… LoRA åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ LoRA åŠ è½½å¤±è´¥: {e}")
    exit(1)

# ============ é—®ç­”å‡½æ•° ============
def ask(question, max_length=200):
    # Qwen å¯¹è¯æ ¼å¼
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant çš„å›ç­”
    if "<|im_start|>assistant" in response:
        answer = response.split("<|im_start|>assistant")[-1].strip()
        return answer
    return response

# ============ æµ‹è¯•é—®é¢˜ ============
test_questions = [
    "ä»€ä¹ˆæ˜¯æ—¶é—´å¤æ‚åº¦ï¼Ÿ",
    "æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯æ­»é”ï¼Ÿ",
    "TCPå’ŒUDPçš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯è™šæ‹Ÿå†…å­˜ï¼Ÿ",
    "è¿›ç¨‹å’Œçº¿ç¨‹çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯äºŒå‰æ ‘ï¼Ÿ",
    "HTTPå’ŒHTTPSçš„åŒºåˆ«"
]

print("\nğŸ“ æµ‹è¯•ç»“æœ:")
print("="*60)

for q in test_questions:
    print(f"ğŸ“Œ é—®é¢˜: {q}")
    answer = ask(q)
    print(f"ğŸ’¬ å›ç­”: {answer}")
    print("-"*40)

# ============ äº¤äº’å¼å¯¹è¯ ============
print("\nğŸ’¬ äº¤äº’å¼å¯¹è¯æ¨¡å¼ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰")
print("="*60)

while True:
    user_input = input("\nä½ : ")
    if user_input.lower() in ['exit', 'quit', 'q']:
        break
    
    answer = ask(user_input)
    print(f"æ¨¡å‹: {answer}")

print("\nâœ… æµ‹è¯•å®Œæˆï¼")